{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## OCOM5203M Assignment 2 - Image Caption Generation [100 marks]\n",
    "\n",
    "The maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 75% of the final grade for the module.\n",
    "\n",
    "**Note:** This notebook includes two figures that are uploaded on the assessment page for viewing and downloading. Ensure that these figures are stored in the same directory as the notebook so they render properly when you load it.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Through this coursework, you will:\n",
    "\n",
    "> 1. Understand the principles of text pre-processing and vocabulary building.\n",
    "> 2. Gain experience working with an image to text model.\n",
    "> 3. Use and compare two different text similarity metrics for evaluating an image to text model, and understand evaluation challenges.\n",
    "\n",
    "\n",
    "### Setup and resources\n",
    "\n",
    "Having a GPU will speed up the image feature extraction process. You can use Google Colab to complete this assingment.\n",
    "\n",
    "Please implement the coursework using Python and PyTorch (i.e., Keras and TensorFlow are **not** accepted).\n",
    "\n",
    "This coursework will use a subset of the [COCO \"Common Objects in Context\" dataset](https://cocodataset.org/) for image caption generation. COCO contains 330K images, of 80 object categories, and at least five textual reference captions per image. Our subset consists of 5029 of these images, each of which has five or more different descriptions of the salient entities and activities, and we will refer to it as COCO_5029.\n",
    "\n",
    "To download the data:\n",
    "\n",
    "> 1. **Images**: download the zip file \"coco_subset_images.zip (812MB)\" [here](https://leeds365-my.sharepoint.com/:f:/g/personal/busmnom_leeds_ac_uk/EuAH3b6a4g9IjTNhroLLXPoB6ho6cwxYSNh885ZzrktYZA?e=QGSYpf).\n",
    "> 2. **Reference captions**: on the COCO [download page](https://cocodataset.org/#download), download the file named \"2017 Train/Val annotations (241MB)\".\n",
    "> 3. **Image meta data**: as our set is a subset of full COCO dataset, we have created a CSV file containing relevant meta data for our particular subset of images. You can download it also from Drive, \"coco_subset_meta.csv\" at the same link as 1.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, in .ipynb format. Please do not submit the figures \n",
    "> 2. The .html version of your notebook. **Check that all cells have been run** and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    ">    \n",
    "**Note:** You can download the HTML version of your notebook in Jupyter Notebook via `File > Download as > HTML (.html)`. If the HTML option is not available in Google Colab, download your `.ipynb` file and either open it in Jupyter Notebook to export as HTML (`File > Download as > HTML (.html)`) or use the command `jupyter nbconvert --to html my-notebook.ipynb`.\r\n",
    "\n",
    "Final note:\n",
    "\n",
    "> **Please include in this notebook everything that you would like to be marked, including figures. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upPwsVQDynPo"
   },
   "source": [
    "Enter your student ID in the cell below. Note that there may be a message on the Minerva submission page asking you to anonymise your submission. Please disregard that message and provide your ID below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poZXcUaIynPp"
   },
   "outputs": [],
   "source": [
    "# Student ID: <your student ID>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp8ekPQ8ynPq"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Feel free to add to this section as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9dg5JQtynPq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9BUR0MdynPr"
   },
   "source": [
    "Detect which device (CPU/GPU) to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2meIfRXIynPr"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sqIve1wynPr"
   },
   "source": [
    "The basic principle of our image-to-text model is as pictured in the diagram below, where an Encoder network encodes the input image as a feature vector by providing the outputs of the last fully-connected layer of a pre-trained CNN (we use [ResNet-152](https://arxiv.org/abs/1512.03385)). This pretrained network has been trained on the complete ImageNet dataset and is thus able to recognise common objects.\n",
    "\n",
    "These features are then fed into a Decoder network along with the reference captions. As the image feature dimensions are large and sparse, the Decoder network includes a linear layer which downsizes them, followed by a batch normalisation layer to speed up training. Those resulting features, as well as the reference text captions, are passed into a recurrent network (we will use an RNN).\n",
    "\n",
    "The reference captions used to compute loss are represented as numerical vectors via an embedding layer whose weights are learned during training.\n",
    "\n",
    "![Encoder Decoder](encoder_decoder_diagramv2022.png)\n",
    "\n",
    "The Encoder-Decoder network could be coupled and trained end-to-end, without saving features to disk; however, this requires iterating through the entire image training set during training. We can make the training more efficient by decoupling the networks.\n",
    "\n",
    "We will first extract the feature representations of the images from the Encoder and save them on disk (Part 1). During training of the Decoder (Part 3), we only need to iterate over the image feature data and the reference captions.\n",
    "\n",
    "### Organisation of the Notebook\n",
    "\n",
    "> 1. Extracting image features [4 marks]\n",
    "> 2. Text preparation of training and validation data [10 marks]\n",
    "> 3. Training the decoder [28 marks]\n",
    "> 4. Generating predictions on test data [8 marks]\n",
    "> 5. Caption evaluation via BLEU score [10 marks]\n",
    "> 6. Caption evaluation via Cosine similarity [17 makrs]\n",
    "> 7. Comparing BLEU and Cosine similarity [20 marks]\n",
    "> 8. Overall quality [3 marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgC2t8XYynPs"
   },
   "source": [
    "## 1 Extracting image features [4 marks]\n",
    "\n",
    "### 1.1 EncoderCNN\n",
    "\n",
    "Read through the template EncoderCNN class below and complete the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Load a pretrained CNN and remove the top fully connected (fc) layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        # TO COMPLETE\n",
    "        # 1. Load the ResNet-152 pretrained model.\n",
    "        # 2. Keep all layers except the final classification layer.\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        \n",
    "        # TO COMPLETE\n",
    "        # 1. Disable gradients for the forward pass through the CNN.\n",
    "        # 2. Pass the images through the modified network.\n",
    "        # 3. Return the extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AY-HCTL8ynPs"
   },
   "outputs": [],
   "source": [
    "# instantiate encoder and put into evaluation mode.\n",
    "encoder = EncoderCNN().to(device)\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkalUTdTynPt"
   },
   "source": [
    "### 1.2 Processing the images\n",
    "\n",
    "Pass the images through the ```Encoder``` model, saving the resulting features for each image. You may like to use a ```Dataset``` and ```DataLoader``` to load the data in batches for faster processing, or you may choose to simply read in one image at a time from disk without any loaders.\n",
    "\n",
    "Note that as this is a forward pass only, no gradients are needed. You will need to be able to match each image ID (the image name without file extension) with its features later, so we suggest either saving a dictionary of image ID: image features, or keeping a separate ordered list of image IDs.\n",
    "\n",
    "Use the following provided ImageNet transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9x-gK_-ynPt"
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ypz0EpogynPt"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22F2djGnynPt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzDuMZD7ynPt"
   },
   "outputs": [],
   "source": [
    "# torch.save(features, 'features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfR--uYXHdIi"
   },
   "source": [
    "## 2 Text preparation [10 marks]\n",
    "\n",
    "\n",
    "### 2.1 Build the caption dataset\n",
    "\n",
    "All our selected COCO_5029 images are from the official 2017 train set.\n",
    "\n",
    "The ```coco_subset_meta.csv``` file includes the image filenames and unique IDs of all the images in our subset. The ```id``` column corresponds to each unique image ID.\n",
    "\n",
    "The COCO dataset includes many different types of annotations: bounding boxes, keypoints, reference captions, and more. We are interested in the captioning labels. Open ```captions_train2017.json``` from the zip file downloaded from the COCO website. You are welcome to come up with your own way of doing it, but we recommend using the ```json``` package to initially inspect the data, then the ```pandas``` package to look at the annotations (if you read in the file as ```data```, then you can access the annotations dictionary as ```data['annotations']```).\n",
    "\n",
    "Use ```coco_subset_meta.csv``` to cross-reference with the annotations from ```captions_train2017.json``` to get all the reference captions for each image in COCO_5029.\n",
    "\n",
    "For example, you may end up with data looking like this (this is a ```pandas``` DataFrame, but it could also be several lists, or some other data structure/s):\n",
    "\n",
    "<img src=\"df_caption_set.png\" alt=\"images matched to caption\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kU7sHz0lynPu"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvQXuZ1-ynPu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2q_aUhBlynPu"
   },
   "source": [
    "### 2.2 Clean the captions\n",
    "\n",
    "Create a cleaned version of each caption. If using dataframes, we suggest saving the cleaned captions in a new column; otherwise, if you are storing your data in some other way, create data structures as needed.\n",
    "\n",
    "**A cleaned caption should be all lowercase, and consist of only alphabet characters.**\n",
    "\n",
    "Print out 10 original captions next to their cleaned versions to facilitate marking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LkT3kP9TynPv"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gb20t08ynPv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4lynzI8ynPw"
   },
   "source": [
    "### 2.3  Split the data\n",
    "\n",
    "Split the data 70/10/20% into train/validation/test sets. **Be sure that each unique image (and all corresponding captions) only appear in a single set.**\n",
    "\n",
    "Complete the provided function below, which takes a list of unique image IDs and a 3-way split ratio as input, and returns a split of the image IDs accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKwNHTCgynPw"
   },
   "outputs": [],
   "source": [
    "def split_ids(image_id_list, train=0.7, valid=0.1, test=0.2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_id_list (list): List of unique image IDs.\n",
    "        train (float): Proportion of the dataset to allocate for training.\n",
    "        valid (float): Proportion of the dataset to allocate for validation.\n",
    "        test (float): Proportion of the dataset to allocate for testing.\n",
    "    Returns:\n",
    "        tuple: Three lists containing IDs for the training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # TO COMPLETE \n",
    "    # 1. Calculate the number of items in each split (train, valid, test).\n",
    "    # 2. Use list slicing to create the splits.\n",
    "    # Think: How might you ensure the sets are representative of the overall dataset?\n",
    "\n",
    "    return train_ids, valid_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Y4dpLVkynPw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building the vocabulary\n",
    "\n",
    "The `Vocabulary` class provided below serves as a simple wrapper to map unique words to integer IDs, facilitating word-to-integer and integer-to-word conversions.\n",
    "\n",
    "1. **Complete the `add_word` Method:** Implement the logic to add a new word to the vocabulary. Ensure that each word is assigned a unique integer ID, starting from `3` (after the special tokens).\n",
    "\n",
    "2. **Complete the `__call__` Method:** Implement the logic to retrieve the integer ID for a given word. If the word is not found in the vocabulary, return the ID for the `<unk>` token.\n",
    "\n",
    "Refer to the class template below and complete the indicated sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5F0j_5DynPx"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    def __init__(self):\n",
    "        # intially, set both the IDs and words to dictionaries with special tokens\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1, '<end>': 2}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<end>'}\n",
    "        self.idx = 3\n",
    "\n",
    "    # TO COMPLETE\n",
    "    # Implement the logic to add a word to the vocabulary\n",
    "    def add_word(self, word):\n",
    "        pass\n",
    "\n",
    "    # TO COMPLETE\n",
    "    # Implement logic to retrieve the integer ID for a word\n",
    "    # Return <unk>'s ID if the word is not found\n",
    "    def __call__(self, word):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16yl-9IgynPx"
   },
   "source": [
    "Collect all words from the cleaned captions in the **training and validation sets**, ignoring any words which appear 3 times or less; this should leave you with roughly 2200 words (plus or minus is fine). As the vocabulary size affects the embedding layer dimensions, it is better not to add the very infrequently used words to the vocabulary.\n",
    "\n",
    "Create an instance of the ```Vocabulary()``` object and add all your words to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f-DhX0WynPx"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Up3MoGAGynPx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIH9Hu1DynPy"
   },
   "source": [
    "### 2.5 The Dataset and DataLoader\n",
    "\n",
    "Create a PyTorch ```Dataset``` class and a corresponding ```DataLoader``` for the inputs to the decoder. Create three sets: one each for training, validation, and test. Set ```shuffle=True``` for the training set DataLoader.\n",
    "\n",
    "The ```Dataset``` function ```__getitem__(self, index)``` should return three Tensors:\n",
    "\n",
    ">1. A Tensor of image features, dimension (1, 2048).\n",
    ">2. A Tensor of integer word ids representing the reference caption; use your ```Vocabulary``` object to convert each word in the caption to a word ID. Be sure to add the word ID for the ```<end>``` token at the end of each caption, then fill in the the rest of the caption with the ```<pad>``` token so that each caption has uniform lenth (max sequence length) of **47**.\n",
    ">3. A Tensor of integers representing the true lengths of every caption in the batch (include the ```<end>``` token in the count).\n",
    "\n",
    "\n",
    "Note that as each unique image has five or more (say, ```n```) reference captions, each image feature will appear ```n``` times, once in each unique (feature, caption) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uM39N--YynPy"
   },
   "outputs": [],
   "source": [
    "class COCO_Subset(Dataset):\n",
    "    \"\"\" COCO subset custom dataset, compatible with torch.utils.data.DataLoader. \"\"\"\n",
    "\n",
    "    def __init__(self, df, features, vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: (dataframe or some other data structure/s you may prefer to use)\n",
    "            features: image features\n",
    "            vocab: vocabulary wrapper\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # TO COMPLETE\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns one data tuple (feature [1, 2048], target caption of word IDs [1, 47], and integer true caption length) \"\"\"\n",
    "\n",
    "       # TO COMPLETE\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCVDmjmuynPy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jP-drJKeynPy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGO8cbdQynQC"
   },
   "source": [
    "Load one batch of the training set and print out the shape of each returned Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAh9hKjxynQC"
   },
   "outputs": [],
   "source": [
    "# train_iter = iter(train_loader)\n",
    "# features, captions, lengths = train_iter.next()\n",
    "# print(features.shape)\n",
    "# print(captions.shape)\n",
    "# print(lengths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj6vQvoSynQD"
   },
   "source": [
    "## 3 Train DecoderRNN [28 marks]\n",
    "\n",
    "### 3.1 Define the decoder model\n",
    "\n",
    "In this section, you will complete the implementation of the `DecoderRNN` class by defining the missing components in the `__init__` and `forward` methods.\n",
    "\n",
    "1. **Initialize the Decoder:**\n",
    "   - Add the required layers such as a linear transformation, batch normalization, an embedding layer, and an RNN.\n",
    "\n",
    "2. **Forward Pass:**\n",
    "   - Implement the logic for processing image features and tokenized captions through the decoder. \n",
    "   - Ensure proper sequence handling with `pack_padded_sequence()`.\n",
    "\n",
    "3. **Answer Questions:**\n",
    "   - In a separate cell given below, answer all the questions embedded in the  ode comments. Use m. explanations.\n",
    "\n",
    "Refer to the provided comments and [PyTorch documentation](https://pytorch.org/docs/stable/index.html) as needed. The code comments outline the functionality for each step, guiding your implementation.mplementation.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=1, max_seq_length=47):\n",
    "        \"\"\"\n",
    "        Initialize the DecoderRNN.\n",
    "        Inputs:\n",
    "        - vocab_size: Size of the vocabulary (number of unique words in captions)\n",
    "        - embed_size: Dimensionality of word embeddings (size of W_emb in the schematic)\n",
    "        - hidden_size: Size of the RNN's hidden state\n",
    "        - num_layers: Number of layers in the RNN (you may or may not use this)\n",
    "        - max_seq_length: Maximum length of the generated captions\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # RECALL: Revisit the schematic of the encoder/decoder architecture given above and in the assessment page.\n",
    "        # The input to the DecoderRNN is the image feature vector from the EncoderCNN (size: 2048).\n",
    "        # The Decoder first transforms this vector using a linear layer followed by a batch normalization layer.\n",
    "\n",
    "        # THINK: What should be the input and output dimensions of this layer? Why?\n",
    "\n",
    "        # QUESTION 1: Why do we need to transform the 2048-dimensional image feature vector before passing it to the RNN?\n",
    "\n",
    "        # QUESTION 2: Why might batch normalization be useful after resizing the image \n",
    "        # feature vector? What potential issues could arise if batch normalization is omitted?\n",
    "\n",
    "        # Define the Linear transformation (Linear + BN)\n",
    "        # TO COMPLETE <your code>\n",
    "\n",
    "        # RECALL: The forward pass of the decoder RNN requires an embedding representation of \n",
    "        # the input tokens/words.\n",
    "\n",
    "        # Read about the `torch.nn.Embedding` class and implement this embedding layer.\n",
    "        # TO COMPLETE <your code>\n",
    "\n",
    "        # QUESTION 3: What is the purpose of the embedding layer in the DecoderRNN? \n",
    "        # Could you use a pretrained word embedding instead of training it from scratch? \n",
    "        # If so, what are the potential advantages and disadvantages of using a pretrained embedding?\n",
    "\n",
    "        # Define the RNN architecture here using hidden units you deem appropriate \n",
    "        # You may experiment with different hidden unit types and number of layers.\n",
    "        self.rnn = # TO COMPLETE <your code>\n",
    "\n",
    "        # QUESTION 4:\n",
    "        # Overfitting and underfitting are common issues when training machine learning models.\n",
    "        # \n",
    "        # Answer the following:\n",
    "        #\n",
    "        # Q4.1. What architectural choices in your RNN design (e.g., number of hidden units, layers, or type of RNN) \n",
    "        # could contribute to overfitting/underfitting? Explain your reasoning.\n",
    "        #\n",
    "        # Q4.2. How would you evaluate whether your model is overfitting or underfitting the data? What metrics or observations \n",
    "        # would you look for during training?\n",
    "        #\n",
    "        # Q4.3. What strategies or techniques would you use to guard against overfitting and underfitting in your model? \n",
    "\n",
    "        # RECALL: The RNN processes sequences (image features + word embeddings) \n",
    "        # and produces hidden states that represent the context of the caption.\n",
    "\n",
    "        # Finally, the output of the RNN hidden state needs to be mapped and converted \n",
    "        # to probabilities over the vocabulary.\n",
    "        # TO COMPLETE <your code>\n",
    "\n",
    "        # Maximum sequence length\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"\n",
    "        Decode image feature vectors and generate captions.\n",
    "        Inputs:\n",
    "        - features: Image features extracted from the EncoderCNN.\n",
    "        - captions: Padded tokenized captions.\n",
    "        - lengths: Actual lengths of captions before padding. You will need this for the pack_padded_sequence function below.\n",
    "        Outputs:\n",
    "        - outputs: Vocabulary predictions for each time step.\n",
    "        \"\"\"\n",
    "    \n",
    "        # STEP 1: Transform the image features using the Linear + BN layer you defined earlier\n",
    "        # Use the transformations you implemented in the __init__ function.\n",
    "        im_features = # TO COMPLETE <your code>\n",
    "    \n",
    "        # STEP 2: Convert captions to embeddings using the embedding layer you defined above\n",
    "        # Use the embedding layer you defined earlier to process `captions`.\n",
    "        embeddings = # TO COMPLETE <your code>\n",
    "    \n",
    "        # STEP 3: Concatenate image features and word embeddings\n",
    "        # Use torch.cat to concatenate `im_features` with `embeddings`\n",
    "        # Hint: You may need to use the unsqueeze() function on the im_features\n",
    "        embeddings = # TO COMPLETE <your code>\n",
    "    \n",
    "        # NOTE: PyTorch's `pack_padded_sequence`:\n",
    "        # This function removes padding tokens and ensures efficient processing of variable-length sequences by the RNN.\n",
    "        # You must use the `lengths` input to create the `packed` sequence.\n",
    "    \n",
    "        # STEP 4: Consult PyTorch's documentation to learn about how to use `pack_padded_sequence`\n",
    "        # to create a variable called `packed` that the RNN will process in the next step.\n",
    "        packed = # TO COMPLETE <your code>\n",
    "    \n",
    "        # STEP 5: Processing the packed sequences with the RNN\n",
    "        hiddens, _ = self.rnn(packed)\n",
    "    \n",
    "        # STEP 6: Transform the hidden states to output vocabulary probabilities\n",
    "        # Use the Linear transformation(s) you defined earlier to map hidden states to vocabulary size.\n",
    "        outputs = # TO COMPLETE <your code>\n",
    "    \n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8HnQbHlynQD"
   },
   "outputs": [],
   "source": [
    "# instantiate decoder\n",
    "# You may experiment with different values for the embedding size, hidden size, etc (checked the default values in function signature above).\n",
    "decoder = DecoderRNN(len(vocab)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMQ3JbdAynQD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers to Questions 1-4\n",
    "Use this cell to answer questions 1-4 in the `DecoderRNN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjVqlScvynQE"
   },
   "source": [
    "### 3.2 Train the decoder\n",
    "\n",
    "Train the decoder by passing the features, reference captions, and targets to the decoder, then computing loss based on the outputs and the targets. Note that when passing the targets and model outputs to the loss function, the targets will also need to be formatted using ```pack_padded_sequence()```.\n",
    "\n",
    "We recommend a batch size of around 64 (though feel free to adjust as necessary for your hardware).\n",
    "\n",
    "We recommend saving a checkpoint of your trained model after training so you don't need to re-train multiple times.\n",
    "\n",
    "Display a graph of training and validation loss over epochs to justify your stopping point. Additionally, document the different RNN units and design choices you experimented with, including the rationale behind each choice. Describe the configurations tested, the observations made during training and validation, and how these observations influenced subsequent changes to your model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gdw_aKsVynQE"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0jBsyZKynQE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXzlLbstCE7f"
   },
   "source": [
    "## 4 Generate predictions on test data [8 marks]\n",
    "\n",
    "Display 5 sample test images containing different objects, along with your model’s generated captions and all the reference captions for each.\n",
    "\n",
    "> Remember that everything **displayed** in the submitted notebook and .html file will be marked, so be sure to run all relevant cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHiqYL3aynQF"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEpmnStFynQF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVfEgbC4I_dE"
   },
   "source": [
    "## 5 Caption evaluation using BLEU score [10 marks]\n",
    "\n",
    "There are different methods for measuring the performance of image to text models. We will evaluate our model by measuring the text similarity between the generated caption and the reference captions, using two commonly used methods. Ther first method is known as *Bilingual Evaluation Understudy (BLEU)*.\n",
    "\n",
    "###  5.1 BLEU score\n",
    "\n",
    "\n",
    "One common way of comparing a generated text to a reference text is using BLEU. This article gives a good intuition to how the BLEU score is computed: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/, and you may find an implementation online to use. One option is the NLTK implementation `nltk.translate.bleu_score` here: https://www.nltk.org/api/nltk.translate.bleu_score.html\n",
    "\n",
    "\n",
    "> **Tip:** BLEU scores can be weighted by ith-gram. Check that your scores make sense; and feel free to use a weighting that best matches the data. We will not be looking for specific score ranges; rather we will check that the scores are reasonable and meaningful given the captions.\n",
    "\n",
    "Write the code to evaluate the trained model on the complete test set and calculate the BLEU score using the predictions, compared against all five references captions.\n",
    "\n",
    "Display a histogram of the distribution of scores over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xypfUN7y4CKI"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_eEwfDbynQG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ANtlq-GynQH"
   },
   "source": [
    "### 5.2 BLEU score examples\n",
    "\n",
    "Find one sample with high BLEU score and one with a low score, and display the model's predicted sentences, the BLEU scores, and the 5 reference captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5NUmdtQynQH"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGswaWVcynQH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VQKNo384CKP"
   },
   "source": [
    "## 6 Caption evaluation using cosine similarity [17 marks]\n",
    "\n",
    "###  6.1 Cosine similarity\n",
    "\n",
    "The cosine similarity measures the cosine of the angle between two vectors in n-dimensional space. The smaller the angle, the greater the similarity.\n",
    "\n",
    "To use the cosine similarity to measure the similarity between the generated caption and the reference captions:\n",
    "\n",
    "* Find the embedding vector of each word in the caption\n",
    "* Compute the average vector for each caption\n",
    "* Compute the cosine similarity score between the average vector of the generated caption and average vector of each reference caption\n",
    "* Compute the average of these scores\n",
    "\n",
    "Calculate the cosine similarity using the model's predictions over the whole test set.\n",
    "\n",
    "Display a histogram of the distribution of scores over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZBCg1PvynQH"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMw9krANynQI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svRNnkFhynQI"
   },
   "source": [
    "#### 6.2 Cosine similarity examples\n",
    "\n",
    "Find one sample with high cosine similarity score and one with a low score, and display the model's predicted sentences, the cosine similarity scores, and the 5 reference captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_XdpqTZynQI"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3OlZ-nEynQJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDmwrp-w4CKR"
   },
   "source": [
    "## 7 Comparing BLEU and Cosine similarity [20 marks]\n",
    "\n",
    "### 7.1 Test set distribution of scores\n",
    "\n",
    "Compare the model’s performance on the test set evaluated using BLEU and cosine similarity and discuss some weaknesses and strengths of each method (explain in words, in a text box below).\n",
    "\n",
    "Please note, to compare the average test scores, you need to rescale the Cosine similarity scores [-1 to 1] to match the range of BLEU method [0.0 - 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2O8TZG74CKS"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qigb0A9F4CKV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQ5CB9EbynQL"
   },
   "source": [
    " ### 7.2 Analysis of individual examples\n",
    "\n",
    "Find and display one example where both methods give similar scores and another example where they do not and discuss. Include both scores, predicted captions, and reference captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YM2hngAynQL"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7gglw4YynQL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Overall quality [3 marks]\n",
    "\n",
    "- **Code Modularity:** The implementation should follow a clean, modular design, with well-structured functions and classes (where approperiate).\n",
    "- **Readability:** Code should be clear with meaningful variable names and appropriate use of comments.\n",
    "- **Output Accessibility:** Outputs should be neatly formatted and easy to interpret, avoiding excessive or unnecessary information dumping into the output.\n",
    "- **Clarity in Visualisation:** Graphs and visualisations results should be well-labelled."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
